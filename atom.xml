<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>卡布诺奇——喵喵喵</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jiebaby.github.io/"/>
  <updated>2020-03-11T03:00:05.333Z</updated>
  <id>https://jiebaby.github.io/</id>
  
  <author>
    <name>Jaybao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>认知计算</title>
    <link href="https://jiebaby.github.io/2020/03/11/%E8%AE%A4%E7%9F%A5%E8%AE%A1%E7%AE%97/"/>
    <id>https://jiebaby.github.io/2020/03/11/%E8%AE%A4%E7%9F%A5%E8%AE%A1%E7%AE%97/</id>
    <published>2020-03-11T02:50:43.000Z</published>
    <updated>2020-03-11T03:00:05.333Z</updated>
    
    <content type="html"><![CDATA[<h3 id="SamduhL"><a href="#SamduhL" class="headerlink" title="SamduhL"></a>SamduhL</h3><p>###　学科<br>认知科学，计算机学</p><p>认知计算是认知科学的核心技术子领域之一，是人工智能的重要组成部分，是模拟人脑认知过程的计算机系统。<br>认知计算代表一种全新的计算模式，它包含信息分析，自然语言处理和机器学习领域的大量技术创新，能够助力决策者从大量非结构化数据中揭示非凡的洞察。认知系统能够以对人类而言更加自然的方式与人类交互；认知系统专门获取海量的不同类型的数据，根据信息进行推论；从自身与数据、与人们的交互中学习。</p><p>###　认知计算</p><p>认知计算最简单的工作是说话、听、看、写,复杂的工作是辅助、理解、决策和发现.认知计算是一种自上而下的、全局性的统一理论研究,旨在解释观察到的认知现象(思维),符合已知的自下而上的神经生物学事实(脑),可以进行计算,也可以用数学原理解释。它寻求一种符合已知的有着脑神经生物学基础的计算机科学类的软、硬件元件,并用于处理感知、记忆、语言、智力和意识等心智过程。认知计算的一个目标是让计算机系统能够像人的大脑一样学习、思考，并做出正确的决策。人脑与电脑各有所长，认知计算系统可以成为一个很好的辅助性工具，配合人类进行工作，解决人脑所不擅长解决的一些问题。<br>认知计算源自模拟人脑的计算机系统的人工智能，90年代后，研究人员开始用认知计算一词，以表明该学科用于教计算机像人脑一样思考，而不只是开发一种人工系统。传统的计算技术是定量的，并着重于精度和序列等级，而认知计算则试图解决生物系统中的不精确、不确定和部分真实的问题，以实现不同程度的感知、记忆、学习、语言、思维和问题解决等过程。<br>随着科学技术的发展以及大数据时代的到来，如何实现类似人脑的认知与判断，发现新的关联和模式，从而做出正确的决策，显得尤为重要，这给认知计算技术的发展带来了新的机遇和挑战。<br>另据IDC预测，到2018年，超过一半的消费者将获取到基于认知计算开发的服务。到2020年，50%的商业分析软件将包含基于认知计算功能的分析工具，同时认知服务将嵌入新的应用之中。嵌入式的数据分析将为美国企业提供超过600亿美元的减省。</p><p>###　关于认知计算与人工智能的关系？</p><p>虽然认知计算包括部分人工智能领域的元素，但是它涉及的范围更广。认知计算不是要生产出代替人类进行思考的机器，而是要放大人类智能，帮助人类更好地思考。<br>认知计算与人工智能，一个更偏向于技术体系，一个更偏向于最终的应用形态。认知计算的渗透，让更多的产品与服务具备了智能，而认知计算本身也是在向人脑致敬，所以双方不仅不矛盾，反而是相辅相成的。<br>长期以来，人工智能研究者都在开发旨在提升计算机性能的技术，这些技术能让计算机完成非常广泛的任务，而这些任务在过去被认为只有人才能完成，包括玩游戏、识别人脸和语音，在不确定的情况下做出决策、学习和翻译语言。<br>IBM Watson 是认知计算系统的杰出代表，也是一个技术平台。认知计算代表一种全新的计算模式，它包含信息分析，自然语言处理和机器学习领域的大量技术创新，能够助力决策者从大量结构化和非结构化数据中揭示非凡的洞察。<br>另外，人们比较熟悉的人工智能产品如Siri。Siri是一款内建在苹果iOS系统中的人工智能助理软件。此软件使用自然语言处理技术，使用者可以使用自然的对话与手机进行互动，完成搜寻资料、查询天气、设定手机日历、设定闹铃等许多服务。</p><p>###　认知商业</p><p>数字商业和人工智能的结合，就形成了认知商业。在认知商业中，认知技术将带给企业和各个行业以全新的变革，引领行业企业转型，包括取代一部分人类的工作，以及重新定义一些工作的内容，让人与机器能够更深度、顺畅的工作，从而产生更大的商业价值。<br>在接下来的3到5年中，认知技术有可能会给职业、工人和公司带来深远的影响。这些技术可以，并且即将消除部分人类的工作。同时，这些技术也有可能被用来重新定义人类的工作。为工人创造出新的机会、为商业公司和客户带来更高的价值。<br>MIT的经济学家David Autor鉴定了几种计算机暂时无法取代的任务所拥有的技能。比如说解决问题的能力、直觉、创造力、说服力——这些是完成所谓“抽象”任务所需要的；还有对场景的适应力、视觉和语言认知力、及人与人之间的互动，即“纯人力任务”所需要的。因此产品设计、服务、娱乐、或者构建使人高兴的环境这些工作都不会在短期内被计算机取代，创造性技能将会变得越来越有价值。<br>根据IBM发布的相关报告，认知商业有5大突出的优势：<br>· 个性化连接：通过了解用户的背景和个性，认知商业可以帮助企业和用户形成智能的连接，为用户提供更加个性化的服务。<br>· 提升专业度：通过向企业引入将最前端领先的知识，认知商业能够提升企业的专业程度。<br>· 持续学习完善的产品与服务：在不断与用户沟通的过程中，认知商务可以开发出持续自主学习和完善的产品与服务。<br>· 加速产品研发与上市：通过洞察传统数据与非结构化数据中的模式，认知商务能够加速高风险的研发，并缩短产品上市时间。<br>· 缩短企业决策时间：利用传统和非结构化数据提炼洞察，认知商务可以缩短企业决策时间与成本，提升决策质量。<br>随着认知技术不断发展并不断演化出新应用，它们往往被用来配合工作，帮助工人提高生产效率并得到更好的结果。领袖们应当想办法让人类参与其中，而不是想当然地认为最好的认知技术应用是完全消除人类劳动力。他们也应当发掘一些能弥补技能短缺的认知技术能力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;SamduhL&quot;&gt;&lt;a href=&quot;#SamduhL&quot; class=&quot;headerlink&quot; title=&quot;SamduhL&quot;&gt;&lt;/a&gt;SamduhL&lt;/h3&gt;&lt;p&gt;###　学科&lt;br&gt;认知科学，计算机学&lt;/p&gt;
&lt;p&gt;认知计算是认知科学的核心技术子领域之一，是人工
      
    
    </summary>
    
    
    
      <category term="科学 知识 技术" scheme="https://jiebaby.github.io/tags/%E7%A7%91%E5%AD%A6-%E7%9F%A5%E8%AF%86-%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>精准推送</title>
    <link href="https://jiebaby.github.io/2020/03/10/%E7%B2%BE%E5%87%86%E6%8E%A8%E9%80%81/"/>
    <id>https://jiebaby.github.io/2020/03/10/%E7%B2%BE%E5%87%86%E6%8E%A8%E9%80%81/</id>
    <published>2020-03-10T15:02:52.000Z</published>
    <updated>2020-03-11T02:47:48.909Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基于内容推荐算法"><a href="#基于内容推荐算法" class="headerlink" title="基于内容推荐算法"></a>基于内容推荐算法</h3><p>基于用户感兴趣的物品A，找到和A内容信息相近的物品B</p><p>（1）找到物品A的内容信息</p><p>（2）找到与内容信息相近的物品B</p><p>运用：这种推荐算法多数运用在简单的推荐列表上，当用户看了物品A立刻展示推荐关联的物品B，不需要通过大量计算反馈。但由于其局限性并不能精准推荐出用户所喜欢的内容。</p><h3 id="基于用户的协同过滤算法（UserCF）"><a href="#基于用户的协同过滤算法（UserCF）" class="headerlink" title="基于用户的协同过滤算法（UserCF）"></a>基于用户的协同过滤算法（UserCF）</h3><p>这种算法给用户推荐和他兴趣相似的其他用户喜欢的物品。</p><p>基于用户的协同过滤算法主要包括两个步骤：</p><p>（1）找到和目标用户兴趣相似的用户集合。</p><p>（2）找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。</p><p>运用：UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，即更社会化，反映了用户所在的小型兴趣群体中物品的热门程度</p><h3 id="基于物品的协同过滤算法（ItemCF）"><a href="#基于物品的协同过滤算法（ItemCF）" class="headerlink" title="基于物品的协同过滤算法（ItemCF）"></a>基于物品的协同过滤算法（ItemCF）</h3><p>这种算法给用户推荐和他之前喜欢的物品相似的物品。</p><p>基于物品的协同过滤算法主要分为两步：</p><p>（1）计算物品之间的相似度。</p><p>（2）运用：ItemCF的推荐结果着重于维系用户的历史兴趣，即更个性化，反映了用户自己的兴趣传承</p><h3 id="隐语义模型算法（LFM）"><a href="#隐语义模型算法（LFM）" class="headerlink" title="隐语义模型算法（LFM）"></a>隐语义模型算法（LFM）</h3><p>通过隐含特征联系用户兴趣和物品</p><p>LFM是一种基于机器学习的方法，具有比较好的理论基础。这个方法和基于邻域的方法相比有更强的理论基础、离线计算空间、时间的复杂度，并且可以实现在线实时推荐。</p><h3 id="其他推荐算法"><a href="#其他推荐算法" class="headerlink" title="其他推荐算法"></a>其他推荐算法</h3><p>1）基于图的推荐算法</p><p>其基本思想是将用户行为数据表示为一系列的二元组。基于用户行为二分图，给用户u推荐物品，可以转化为计算用户顶点u和与所有物品顶点i之间的相关性，然后取与用户没有直接边相连的物品，按照相关性的高低生成推荐列表。</p><p>2）基于关联规则的推荐</p><p>反映一个事物与其他事物之间的相互依存性和关联性，常用于实体商店或在线电商的推荐系统：通过对顾客的购买记录数据库进行关联规则挖掘，最终目的是发现顾客群体的购买习惯的内在共性。</p><p>3）基于知识推荐</p><p>使用用户知识和产品知识, 通过推理什么产品能满足用户需求来产生推荐。这种推荐系统不依赖于用户评分等关于用户偏好的历史数据, 故其不存在冷启动方面的问题。基于知识的推荐系统响应用户的即时需求, 当用户偏好发生变化时不需要任何训练。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;基于内容推荐算法&quot;&gt;&lt;a href=&quot;#基于内容推荐算法&quot; class=&quot;headerlink&quot; title=&quot;基于内容推荐算法&quot;&gt;&lt;/a&gt;基于内容推荐算法&lt;/h3&gt;&lt;p&gt;基于用户感兴趣的物品A，找到和A内容信息相近的物品B&lt;/p&gt;
&lt;p&gt;（1）找到物品A的内容信
      
    
    </summary>
    
    
    
      <category term="技术 算法 科技" scheme="https://jiebaby.github.io/tags/%E6%8A%80%E6%9C%AF-%E7%AE%97%E6%B3%95-%E7%A7%91%E6%8A%80/"/>
    
  </entry>
  
  <entry>
    <title>邻近算法</title>
    <link href="https://jiebaby.github.io/2020/03/10/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/"/>
    <id>https://jiebaby.github.io/2020/03/10/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/</id>
    <published>2020-03-10T07:38:29.000Z</published>
    <updated>2020-03-11T02:47:49.969Z</updated>
    
    <content type="html"><![CDATA[<p>邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。<br>kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 kNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。</p><p>K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 KNN方法虽然从原理上也依赖于极限定理，但在类别决策时，只与极少量的相邻样本有关。由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。<br>KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成反比。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。&lt;br&gt;kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样
      
    
    </summary>
    
    
    
      <category term="学习 技术" scheme="https://jiebaby.github.io/tags/%E5%AD%A6%E4%B9%A0-%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning</title>
    <link href="https://jiebaby.github.io/2020/03/10/Machine-Learning/"/>
    <id>https://jiebaby.github.io/2020/03/10/Machine-Learning/</id>
    <published>2020-03-10T07:37:11.000Z</published>
    <updated>2020-03-11T02:47:42.085Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。<br>它是人工智能的核心，是使计算机具有智能的根本途径。</p><p>机器学习是一门多学科交叉专业，涵盖概率论知识，统计学知识，近似理论知识和复杂算法知识，使用计算机作为工具并致力于真实实时的模拟人类学习方式， 并将现有内容进行知识结构划分来有效提高学习效率。 [1]<br>机器学习有下面几种定义：<br>(1) 机器学习是一门人工智能的科学,该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。<br>(2) 机器学习是对能通过经验自动改进的计算机算法的研究。<br>(3) 机器学习是用数据或以往的经验,以此优化计算机程序的性能标准</p><h3 id="决策树-decision-tree-是一种基本的分类与回归方法"><a href="#决策树-decision-tree-是一种基本的分类与回归方法" class="headerlink" title="决策树(decision tree)是一种基本的分类与回归方法"></a>决策树(decision tree)是一种基本的分类与回归方法</h3><p>决策树算法的核心在于决策树的构建，每次选择让整体数据香农熵（描述数据的混乱程度）减小最多的特征，使用其特征值对数据进行划分，每次消耗一个特征，不断迭代分类，直到所有特征消耗完（选择剩下数据中出现次数最多的类别作为这堆数据的类别），或剩下的数据全为同一类别，不必继续划分，至此决策树构建完成，之后我们依照这颗决策树对新进数据进行分类。</p><h3 id="结点和模块的概念"><a href="#结点和模块的概念" class="headerlink" title="结点和模块的概念"></a>结点和模块的概念</h3><p> 一个决策树，长方形代表判断模块(decision block)，椭圆形成代表终止模块(terminating block)，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支(branch)，它可以达到另一个判断模块或者终止模块。我们还可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。如图所示的决策树，长方形和椭圆形都是结点。长方形的结点属于内部结点，椭圆形的结点属于叶结点，从结点引出的左右箭头就是有向边。而最上面的结点就是决策树的根结点(root node)。</p><h3 id="使用决策树做预测需要的过程"><a href="#使用决策树做预测需要的过程" class="headerlink" title="使用决策树做预测需要的过程"></a>使用决策树做预测需要的过程</h3><p>收集数据：可以使用任何方法。比如想构建一个相亲系统，我们可以从媒婆那里，或者通过参访相亲对象获取数据。根据他们考虑的因素和最终的选择结果，就可以得到一些供我们利用的数据了。<br>准备数据：收集完的数据，我们要进行整理，将这些所有收集的信息按照一定规则整理出来，并排版，方便我们进行后续处理。<br>分析数据：可以使用任何方法，决策树构造完成之后，我们可以检查决策树图形是否符合预期。<br>训练算法：这个过程也就是构造决策树，同样也可以说是决策树学习，就是构造一个决策树的数据结构。<br>测试算法：使用经验树计算错误率。当错误率达到了可接收范围，这个决策树就可以投放使用了。<br>使用算法：此步骤可以使用适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。</p><h3 id="编写代码计算经验熵"><a href="#编写代码计算经验熵" class="headerlink" title="编写代码计算经验熵"></a>编写代码计算经验熵</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">from math import <span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">函数说明:创建测试数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    dataSet - 数据集</span></span><br><span class="line"><span class="string">    labels - 分类属性</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-07-20</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line">def createDataSet():</span><br><span class="line">    dataSet = [[0, 0, 0, 0, <span class="string">'no'</span>],         <span class="comment">#数据集</span></span><br><span class="line">            [0, 0, 0, 1, <span class="string">'no'</span>],</span><br><span class="line">            [0, 1, 0, 1, <span class="string">'yes'</span>],</span><br><span class="line">            [0, 1, 1, 0, <span class="string">'yes'</span>],</span><br><span class="line">            [0, 0, 0, 0, <span class="string">'no'</span>],</span><br><span class="line">            [1, 0, 0, 0, <span class="string">'no'</span>],</span><br><span class="line">            [1, 0, 0, 1, <span class="string">'no'</span>],</span><br><span class="line">            [1, 1, 1, 1, <span class="string">'yes'</span>],</span><br><span class="line">            [1, 0, 1, 2, <span class="string">'yes'</span>],</span><br><span class="line">            [1, 0, 1, 2, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 0, 1, 2, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 0, 1, 1, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 1, 0, 1, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 1, 0, 2, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 0, 0, 0, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'年龄'</span>, <span class="string">'有工作'</span>, <span class="string">'有自己的房子'</span>, <span class="string">'信贷情况'</span>]        <span class="comment">#分类属性</span></span><br><span class="line">    <span class="built_in">return</span> dataSet, labels                <span class="comment">#返回数据集和分类属性</span></span><br><span class="line"></span><br><span class="line"><span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">函数说明:计算给定数据集的经验熵(香农熵)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 数据集</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    shannonEnt - 经验熵(香农熵)</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line">def calcShannonEnt(dataSet):</span><br><span class="line">    numEntires = len(dataSet)                        <span class="comment">#返回数据集的行数</span></span><br><span class="line">    labelCounts = &#123;&#125;                                <span class="comment">#保存每个标签(Label)出现次数的字典</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:                            <span class="comment">#对每组特征向量进行统计</span></span><br><span class="line">        currentLabel = featVec[-1]                    <span class="comment">#提取标签(Label)信息</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel not <span class="keyword">in</span> labelCounts.keys():    <span class="comment">#如果标签(Label)没有放入统计次数的字典,添加进去</span></span><br><span class="line">            labelCounts[currentLabel] = 0</span><br><span class="line">        labelCounts[currentLabel] += 1                <span class="comment">#Label计数</span></span><br><span class="line">    shannonEnt = 0.0                                <span class="comment">#经验熵(香农熵)</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:                            <span class="comment">#计算香农熵</span></span><br><span class="line">        prob = <span class="built_in">float</span>(labelCounts[key]) / numEntires    <span class="comment">#选择该标签(Label)的概率</span></span><br><span class="line">        shannonEnt -= prob * <span class="built_in">log</span>(prob, 2)            <span class="comment">#利用公式计算</span></span><br><span class="line">    <span class="built_in">return</span> shannonEnt                                <span class="comment">#返回经验熵(香农熵)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dataSet, features = createDataSet()</span><br><span class="line">    <span class="built_in">print</span>(dataSet)</span><br><span class="line">    <span class="built_in">print</span>(calcShannonEnt(dataSet))</span><br></pre></td></tr></table></figure><h3 id="编写代码计算信息增益"><a href="#编写代码计算信息增益" class="headerlink" title="编写代码计算信息增益"></a>编写代码计算信息增益</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$#</span> -*- coding: UTF-8 -*-</span><br><span class="line">from math import <span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">函数说明:计算给定数据集的经验熵(香农熵)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 数据集</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    shannonEnt - 经验熵(香农熵)</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-03-29</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line">def calcShannonEnt(dataSet):</span><br><span class="line">    numEntires = len(dataSet)                        <span class="comment">#返回数据集的行数</span></span><br><span class="line">    labelCounts = &#123;&#125;                                <span class="comment">#保存每个标签(Label)出现次数的字典</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:                            <span class="comment">#对每组特征向量进行统计</span></span><br><span class="line">        currentLabel = featVec[-1]                    <span class="comment">#提取标签(Label)信息</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel not <span class="keyword">in</span> labelCounts.keys():    <span class="comment">#如果标签(Label)没有放入统计次数的字典,添加进去</span></span><br><span class="line">            labelCounts[currentLabel] = 0</span><br><span class="line">        labelCounts[currentLabel] += 1                <span class="comment">#Label计数</span></span><br><span class="line">    shannonEnt = 0.0                                <span class="comment">#经验熵(香农熵)</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:                            <span class="comment">#计算香农熵</span></span><br><span class="line">        prob = <span class="built_in">float</span>(labelCounts[key]) / numEntires    <span class="comment">#选择该标签(Label)的概率</span></span><br><span class="line">        shannonEnt -= prob * <span class="built_in">log</span>(prob, 2)            <span class="comment">#利用公式计算</span></span><br><span class="line">    <span class="built_in">return</span> shannonEnt                                <span class="comment">#返回经验熵(香农熵)</span></span><br><span class="line"></span><br><span class="line"><span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">函数说明:创建测试数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    dataSet - 数据集</span></span><br><span class="line"><span class="string">    labels - 分类属性</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-07-20</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line">def createDataSet():</span><br><span class="line">    dataSet = [[0, 0, 0, 0, <span class="string">'no'</span>],                        <span class="comment">#数据集</span></span><br><span class="line">            [0, 0, 0, 1, <span class="string">'no'</span>],</span><br><span class="line">            [0, 1, 0, 1, <span class="string">'yes'</span>],</span><br><span class="line">            [0, 1, 1, 0, <span class="string">'yes'</span>],</span><br><span class="line">            [0, 0, 0, 0, <span class="string">'no'</span>],</span><br><span class="line">            [1, 0, 0, 0, <span class="string">'no'</span>],</span><br><span class="line">            [1, 0, 0, 1, <span class="string">'no'</span>],</span><br><span class="line">            [1, 1, 1, 1, <span class="string">'yes'</span>],</span><br><span class="line">            [1, 0, 1, 2, <span class="string">'yes'</span>],</span><br><span class="line">            [1, 0, 1, 2, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 0, 1, 2, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 0, 1, 1, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 1, 0, 1, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 1, 0, 2, <span class="string">'yes'</span>],</span><br><span class="line">            [2, 0, 0, 0, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'年龄'</span>, <span class="string">'有工作'</span>, <span class="string">'有自己的房子'</span>, <span class="string">'信贷情况'</span>]        <span class="comment">#分类属性</span></span><br><span class="line">    <span class="built_in">return</span> dataSet, labels                             <span class="comment">#返回数据集和分类属性</span></span><br><span class="line"></span><br><span class="line"><span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">函数说明:按照给定特征划分数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 待划分的数据集</span></span><br><span class="line"><span class="string">    axis - 划分数据集的特征</span></span><br><span class="line"><span class="string">    value - 需要返回的特征的值</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-03-30</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line">def splitDataSet(dataSet, axis, value):</span><br><span class="line">    retDataSet = []                                        <span class="comment">#创建返回的数据集列表</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:                             <span class="comment">#遍历数据集</span></span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]                <span class="comment">#去掉axis特征</span></span><br><span class="line">            reducedFeatVec.extend(featVec[axis+1:])     <span class="comment">#将符合条件的添加到返回的数据集</span></span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="built_in">return</span> retDataSet                                      <span class="comment">#返回划分后的数据集</span></span><br><span class="line"></span><br><span class="line"><span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">函数说明:选择最优特征</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 数据集</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    bestFeature - 信息增益最大的(最优)特征的索引值</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line">def chooseBestFeatureToSplit(dataSet):</span><br><span class="line">    numFeatures = len(dataSet[0]) - 1                    <span class="comment">#特征数量</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)                 <span class="comment">#计算数据集的香农熵</span></span><br><span class="line">    bestInfoGain = 0.0                                  <span class="comment">#信息增益</span></span><br><span class="line">    bestFeature = -1                                    <span class="comment">#最优特征的索引值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):                         <span class="comment">#遍历所有特征</span></span><br><span class="line">        <span class="comment">#获取dataSet的第i个所有特征</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)                         <span class="comment">#创建set集合&#123;&#125;,元素不可重复</span></span><br><span class="line">        newEntropy = 0.0                                  <span class="comment">#经验条件熵</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:                         <span class="comment">#计算信息增益</span></span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)         <span class="comment">#subDataSet划分后的子集</span></span><br><span class="line">            prob = len(subDataSet) / <span class="built_in">float</span>(len(dataSet))           <span class="comment">#计算子集的概率</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)     <span class="comment">#根据公式计算经验条件熵</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy                     <span class="comment">#信息增益</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"第%d个特征的增益为%.3f"</span> % (i, infoGain))            <span class="comment">#打印每个特征的信息增益</span></span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):                             <span class="comment">#计算信息增益</span></span><br><span class="line">            bestInfoGain = infoGain                             <span class="comment">#更新信息增益，找到最大的信息增益</span></span><br><span class="line">            bestFeature = i                                     <span class="comment">#记录信息增益最大的特征的索引值</span></span><br><span class="line">    <span class="built_in">return</span> bestFeature                                             <span class="comment">#返回信息增益最大的特征的索引值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dataSet, features = createDataSet()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"最优特征索引值:"</span> + str(chooseBestFeatureToSplit(dataSet)))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。&lt;br&gt;它是人工智能的核心，是使计算机具有智能的根本途径。&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="科技 技术 学习" scheme="https://jiebaby.github.io/tags/%E7%A7%91%E6%8A%80-%E6%8A%80%E6%9C%AF-%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning</title>
    <link href="https://jiebaby.github.io/2020/03/10/Deep-Learning/"/>
    <id>https://jiebaby.github.io/2020/03/10/Deep-Learning/</id>
    <published>2020-03-10T07:35:46.000Z</published>
    <updated>2020-03-11T02:47:37.579Z</updated>
    
    <content type="html"><![CDATA[<p>深度学习(DL, Deep Learning)是机器学习(ML, Machine Learning)领域中一个新的研究方向，它被引入机器学习使其更接近于最初的目标——人工智能(AI, Artificial Intelligence)。<br>深度学习是学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字，图像和声音等数据的解释有很大的帮助。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据。 深度学习是一个复杂的机器学习算法，在语音和图像识别方面取得的效果，远远超过先前相关技术。<br>深度学习在搜索技术，数据挖掘，机器学习，机器翻译，自然语言处理，多媒体学习，语音，推荐和个性化技术，以及其他相关领域都取得了很多成果。深度学习使机器模仿视听和思考等人类的活动，解决了很多复杂的模式识别难题，使得人工智能相关技术取得了很大进步。</p><h2 id="十种深度学习算法要点及代码解析"><a href="#十种深度学习算法要点及代码解析" class="headerlink" title="十种深度学习算法要点及代码解析"></a>十种深度学习算法要点及代码解析</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>Python 代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$#Import</span> Library</span><br><span class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></span><br><span class="line">from sklearn import linear_model</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Load Train and Test datasets</span></span><br><span class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></span><br><span class="line">x_train=input_variables_values_training_datasets</span><br><span class="line">y_train=target_variables_values_training_datasets</span><br><span class="line">x_test=input_variables_values_test_datasets</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Create linear regression object</span></span><br><span class="line">linear = linear_model.LinearRegression()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">linear.fit(x_train, y_train)</span><br><span class="line">linear.score(x_train, y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Equation coefficient and Intercept</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Coefficient: n'</span>, linear.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Intercept: n'</span>, linear.intercept_)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= linear.predict(x_test)</span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Load Train and Test datasets</span></span><br><span class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></span><br><span class="line">x_train &lt;- input_variables_values_training_datasets</span><br><span class="line">y_train &lt;- target_variables_values_training_datasets</span><br><span class="line">x_test &lt;- input_variables_values_test_datasets</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">linear &lt;- lm(y_train ~ ., data = x)</span><br><span class="line">summary(linear)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(linear,x_test)</span><br></pre></td></tr></table></figure><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>别被它的名字迷惑了！这是一个分类算法而不是一个回归算法。该算法可根据已知的一系列因变量估计离散数值（比方说二进制数值 0 或 1 ，是或否，真或假）。简单来说，它通过将数据拟合进一个逻辑函数来预估一个事件出现的概率。因此，它也被叫做逻辑回归。因为它预估的是概率，所以它的输出值大小在 0 和 1 之间（正如所预计的一样）。</p><p>Python代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$#Import</span> Library</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create logistic regression object</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line">model.score(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Equation coefficient and Intercept</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Coefficient: n'</span>, model.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Intercept: n'</span>, model.intercept_)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ x &lt;- cbind(x_train,y_train)</span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">logistic &lt;- glm(y_train ~ ., data = x,family=<span class="string">'binomial'</span>)</span><br><span class="line">summary(logistic)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(logistic,x_test)</span><br></pre></td></tr></table></figure><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>这个监督式学习算法通常被用于分类问题。令人惊奇的是，它同时适用于分类变量和连续因变量。在这个算法中，我们将总体分成两个或更多的同类群。这是根据最重要的属性或者自变量来分成尽可能不同的组别。</p><p>Python代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></span><br><span class="line">from sklearn import tree</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create tree object</span></span><br><span class="line">model = tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>)</span><br><span class="line"><span class="comment"># for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini </span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># model = tree.DecisionTreeRegressor() for regression</span></span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line">model.score(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$library</span>(rpart)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># grow tree</span></span><br><span class="line">fit &lt;- rpart(y_train ~ ., data = x,method=<span class="string">"class"</span>)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure><h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><p>这是一种分类方法。在这个算法中，我们将每个数据在N维空间中用点标出（N是你所有的特征总数），每个特征的值是一个坐标的值。</p><p>举个例子，如果我们只有身高和头发长度两个特征，我们会在二维空间中标出这两个变量，每个点有两个坐标（这些坐标叫做支持向量）。</p><p>Python代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn import svm</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predic</span></span><br><span class="line">tor) and Y (target) <span class="keyword">for</span> training data <span class="built_in">set</span> and x_test(predictor) of test_dataset</span><br><span class="line"><span class="comment"># Create SVM classification object</span></span><br><span class="line">model = svm.svc()</span><br><span class="line"><span class="comment"># there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.</span></span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line">model.score(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$library</span>(e1071)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fit &lt;-svm(y_train ~ ., data = x)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure><h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>在预示变量间相互独立的前提下，根据 贝叶斯定理 可以得到朴素贝叶斯这个分类方法。用更简单的话来说，一个朴素贝叶斯分类器假设一个分类的特性与该分类的其它特性不相关。举个例子，如果一个水果又圆又红 ， 并且直径大约是 3 英寸，那么这个水果可能会是苹果。即便这些特性互相依赖 ， 或者依赖于别的特性的存在，朴素贝叶斯分类器还是会假设这些特性分别独立地暗示这个水果是个苹果。</p><p>朴素贝叶斯模型易于建造，且对于大型数据集非常有用。虽然简单，但是朴素贝叶斯的表现却超越了非常复杂的分类方法。</p><p>贝叶斯定理提供了一种从P(c)、P(x)和P(x|c) 计算后验概率 P(c|x) 的方法。</p><p>Python代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link</span></span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ library(e1071)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fit &lt;-naiveBayes(y_train ~ ., data = x)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure><h3 id="KNN（K-–-最近邻算法）"><a href="#KNN（K-–-最近邻算法）" class="headerlink" title="KNN（K – 最近邻算法）"></a>KNN（K – 最近邻算法）</h3><p>该算法可用于分类问题和回归问题。然而，在业界内，K – 最近邻算法更常用于分类问题。K – 最近邻算法是一个简单的算法。它储存所有的案例，通过周围k个案例中的大多数情况划分新的案例。根据一个距离函数，新案例会被分配到它的 K 个近邻中最普遍的类别中去。</p><p>这些距离函数可以是欧式距离、曼哈顿距离、明式距离或者是汉明距离。前三个距离函数用于连续函数，第四个函数（汉明函数）则被用于分类变量。如果 K=1，新案例就直接被分到离其最近的案例所属的类别中。有时候，使用 KNN 建模时，选择 K 的取值是一个挑战。</p><p>Python代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create KNeighbors classifier object model</span></span><br><span class="line">KNeighborsClassifier(n_neighbors=6)</span><br><span class="line"><span class="comment"># default value for n_neighbors is 5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ library(knn)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fit &lt;-knn(y_train ~ ., data = x,k=5)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure><h3 id="K-均值算法"><a href="#K-均值算法" class="headerlink" title="K 均值算法"></a>K 均值算法</h3><p>K – 均值算法是一种非监督式学习算法，它能解决聚类问题。使用 K – 均值算法来将一个数据归入一定数量的集群（假设有 k 个集群）的过程是简单的。一个集群内的数据点是均匀齐次的，并且异于别的集群。</p><p>还记得从墨水渍里找出形状的活动吗？K – 均值算法在某方面类似于这个活动。观察形状，并延伸想象来找出到底有多少种集群或者总体。</p><p>Python代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset</span></span><br><span class="line"><span class="comment"># Create KNeighbors classifier object model</span></span><br><span class="line">k_means = KMeans(n_clusters=3, random_state=0)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ library(cluster)</span><br><span class="line">fit &lt;- kmeans(X, 3) <span class="comment"># 5 cluster solution</span></span><br></pre></td></tr></table></figure><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>随机森林是表示决策树总体的一个专有名词。在随机森林算法中，我们有一系列的决策树（因此又名“森林”）。为了根据一个新对象的属性将其分类，每一个决策树有一个分类，称之为这个决策树“投票”给该分类。这个森林选择获得森林里（在所有树中）获得票数最多的分类。</p><p>每棵树是像这样种植养成的：</p><p>如果训练集的案例数是 N，则从 N 个案例中用重置抽样法随机抽取样本。这个样本将作为“养育”树的训练集。<br>假如有 M 个输入变量，则定义一个数字 m&lt;&lt;M。m 表示，从 M 中随机选中 m 个变量，这 m 个变量中最好的切分会被用来切分该节点。在种植森林的过程中，m 的值保持不变。<br>尽可能大地种植每一棵树，全程不剪枝。</p><p>Python</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create Random Forest object</span></span><br><span class="line">model= RandomForestClassifier()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ library(randomForest)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fit &lt;- randomForest(Species ~ ., x,ntree=500)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure><h3 id="降维算法"><a href="#降维算法" class="headerlink" title="降维算法"></a>降维算法</h3><p>在过去的 4 到 5 年里，在每一个可能的阶段，信息捕捉都呈指数增长。公司、政府机构、研究组织在应对着新资源以外，还捕捉详尽的信息。</p><p>举个例子：电子商务公司更详细地捕捉关于顾客的资料：个人信息、网络浏览记录、他们的喜恶、购买记录、反馈以及别的许多信息，比你身边的杂货店售货员更加关注你。</p><p>Python代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn import decomposition</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have training and test data set as train and test</span></span><br><span class="line"><span class="comment"># Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)</span></span><br><span class="line"><span class="comment"># For Factor analysis</span></span><br><span class="line"><span class="comment">#fa= decomposition.FactorAnalysis()</span></span><br><span class="line"><span class="comment"># Reduced the dimension of training dataset using PCA</span></span><br><span class="line">train_reduced = pca.fit_transform(train)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Reduced the dimension of test dataset</span></span><br><span class="line">test_reduced = pca.transform(<span class="built_in">test</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#For more detail on this, please refer  this link.</span></span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ library(stats)</span><br><span class="line">pca &lt;- princomp(train, cor = TRUE)</span><br><span class="line">train_reduced  &lt;- predict(pca,train)</span><br><span class="line">test_reduced  &lt;- predict(pca,<span class="built_in">test</span>)</span><br></pre></td></tr></table></figure><h3 id="Gradient-Boosting-和-AdaBoost-算法"><a href="#Gradient-Boosting-和-AdaBoost-算法" class="headerlink" title="Gradient Boosting 和 AdaBoost 算法"></a>Gradient Boosting 和 AdaBoost 算法</h3><p>当我们要处理很多数据来做一个有高预测能力的预测时，我们会用到 GBM 和 AdaBoost 这两种 boosting 算法。boosting 算法是一种集成学习算法。它结合了建立在多个基础估计值基础上的预测结果，来增进单个估计值的可靠程度。这些 boosting 算法通常在数据科学比赛如 Kaggl、AV Hackathon、CrowdAnalytix 中很有效。</p><p>Python代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.ensemble import GradientBoostingClassifier</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create Gradient Boosting Classifier object</span></span><br><span class="line">model= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure><p>R代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ library(caret)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fitControl &lt;- trainControl( method = <span class="string">"repeatedcv"</span>, number = 4, repeats = 4)</span><br><span class="line">fit &lt;- train(y ~ ., data = x, method = <span class="string">"gbm"</span>, trControl = fitControl,verbose = FALSE)</span><br><span class="line">predicted= predict(fit,x_test,<span class="built_in">type</span>= <span class="string">"prob"</span>)[,2]</span><br></pre></td></tr></table></figure><p>GradientBoostingClassifier 和随机森林是两种不同的 boosting 树分类器。</p><p>如果你想要掌握机器学习，那就立刻开始吧。做做练习，理性地认识整个过程，应用这些代码，并感受乐趣吧！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;深度学习(DL, Deep Learning)是机器学习(ML, Machine Learning)领域中一个新的研究方向，它被引入机器学习使其更接近于最初的目标——人工智能(AI, Artificial Intelligence)。&lt;br&gt;深度学习是学习样本数据的内在规律
      
    
    </summary>
    
    
    
      <category term="学习 技术" scheme="https://jiebaby.github.io/tags/%E5%AD%A6%E4%B9%A0-%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>Vue.js</title>
    <link href="https://jiebaby.github.io/2020/03/10/Vue-js/"/>
    <id>https://jiebaby.github.io/2020/03/10/Vue-js/</id>
    <published>2020-03-10T07:33:32.000Z</published>
    <updated>2020-03-11T03:00:09.752Z</updated>
    
    <content type="html"><![CDATA[<p>Vue (读音 /vjuː/，类似于 view) 是一套用于构建用户界面的渐进式框架。与其它大型框架不同的是，Vue 被设计为可以自底向上逐层应用。Vue 的核心库只关注视图层，不仅易于上手，还便于与第三方库或既有项目整合。另一方面，当与现代化的工具链以及各种支持类库结合使用时，Vue 也完全能够为复杂的单页应用提供驱动。</p><h3 id="动态设置页面标题"><a href="#动态设置页面标题" class="headerlink" title="动态设置页面标题"></a>动态设置页面标题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">$ import VueRouter from <span class="string">'vue-router'</span>;</span><br><span class="line">...</span><br><span class="line"> </span><br><span class="line">//加载 vue-router 插件</span><br><span class="line">Vue.use(VueRouter);</span><br><span class="line"> </span><br><span class="line">/*定义路由匹配表*/</span><br><span class="line">const Routers = [&#123;</span><br><span class="line"> path: <span class="string">'/index'</span>,</span><br><span class="line"> component: (resolve) =&gt; require([<span class="string">'./router/views/index.vue'</span>], resolve),</span><br><span class="line"> meta: &#123;</span><br><span class="line">  title: <span class="string">'首页'</span></span><br><span class="line"> &#125;</span><br><span class="line">&#125;,</span><br><span class="line"> //一次性加载</span><br><span class="line"> // &#123;</span><br><span class="line"> //  path: <span class="string">'/index'</span>,</span><br><span class="line"> //  component: require(<span class="string">'./router/views/index.vue'</span>)</span><br><span class="line"> // &#125;,</span><br><span class="line"> &#123;</span><br><span class="line">  path: <span class="string">'/about'</span>,</span><br><span class="line">  component: (resolve) =&gt; require([<span class="string">'./router/views/about.vue'</span>], resolve),</span><br><span class="line">  meta: &#123;</span><br><span class="line">   title: <span class="string">'关于'</span></span><br><span class="line">  &#125;</span><br><span class="line"> &#125;,</span><br><span class="line"> &#123;</span><br><span class="line">  path: <span class="string">'/article/:id'</span>,</span><br><span class="line">  component: (resolve) =&gt; require([<span class="string">'./router/views/article.vue'</span>], resolve)</span><br><span class="line"> &#125;</span><br><span class="line"> ,</span><br><span class="line"> &#123;//当访问的页面不存在时，重定向到首页</span><br><span class="line">  path: <span class="string">'*'</span>,</span><br><span class="line">  redirect: <span class="string">'/index'</span></span><br><span class="line"> &#125;</span><br><span class="line">]</span><br><span class="line">//路由配置</span><br><span class="line">const RouterConfig = &#123;</span><br><span class="line"> //使用 HTML5 的 History 路由模式</span><br><span class="line"> mode: <span class="string">'history'</span>,</span><br><span class="line"> routes: Routers</span><br><span class="line">&#125;;</span><br><span class="line">//路由实例</span><br><span class="line">const router = new VueRouter(RouterConfig);</span><br><span class="line">//动态设置页面标题</span><br><span class="line">router.beforeEach((to, from, next) =&gt; &#123;</span><br><span class="line"> window.document.title = to.meta.title;</span><br><span class="line"> next();</span><br><span class="line">&#125;)</span><br><span class="line">new Vue(&#123;</span><br><span class="line"> el: <span class="string">'#app'</span>,</span><br><span class="line"> router: router,</span><br><span class="line"> render: h =&gt; h(Hello)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="长页面跳转自动返回顶端"><a href="#长页面跳转自动返回顶端" class="headerlink" title="长页面跳转自动返回顶端"></a>长页面跳转自动返回顶端</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ router.afterEach((to, from, next) =&gt; &#123;</span><br><span class="line"> window.scrollTo(0, 0);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="next参数"><a href="#next参数" class="headerlink" title="next参数"></a>next参数</h3><p>router.afterEach((to, from, next) =&gt; {window.scrollTo(0, 0);});</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ router.beforeEach((to, from, next) =&gt; &#123;</span><br><span class="line"> <span class="keyword">if</span> (window.localStorage.getItem(<span class="string">'token'</span>)) &#123;</span><br><span class="line">  next();</span><br><span class="line"> &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  next(<span class="string">'/login'</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>next() 入参，如果是 false，会不导航；如果为路径，则会导航到指定路径下的页面。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Vue (读音 /vjuː/，类似于 view) 是一套用于构建用户界面的渐进式框架。与其它大型框架不同的是，Vue 被设计为可以自底向上逐层应用。Vue 的核心库只关注视图层，不仅易于上手，还便于与第三方库或既有项目整合。另一方面，当与现代化的工具链以及各种支持类库结合使
      
    
    </summary>
    
    
    
      <category term="技术  代码  学习" scheme="https://jiebaby.github.io/tags/%E6%8A%80%E6%9C%AF-%E4%BB%A3%E7%A0%81-%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>树莓派</title>
    <link href="https://jiebaby.github.io/2020/03/10/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    <id>https://jiebaby.github.io/2020/03/10/%E6%A0%91%E8%8E%93%E6%B4%BE/</id>
    <published>2020-03-10T07:15:21.000Z</published>
    <updated>2020-03-11T02:47:47.631Z</updated>
    
    <content type="html"><![CDATA[<p>树莓派由注册于英国的慈善组织“Raspberry Pi 基金会”开发，Eben·Upton/埃·厄普顿为项目带头人。2012年3月，英国剑桥大学埃本·阿普顿（Eben Epton）正式发售世界上最小的台式机，又称卡片式电脑，外形只有信用卡大小，却具有电脑的所有基本功能，这就是Raspberry Pi电脑板，中文译名”树莓派”。这一基金会以提升学校计算机科学及相关学科的教育，让计算机变得有趣为宗旨。基金会期望这 一款电脑无论是在发展中国家还是在发达国家，会有更多的其它应用不断被开发出来，并应用到更多领域。在2006年树莓派早期概念是基于Atmel的 ATmega644单片机，首批上市的10000“台”树莓派的“板子”，由中国台湾和大陆厂家制造。<br>它是一款基于ARM的微型电脑主板，以SD/MicroSD卡为内存硬盘，卡片主板周围有1/2/4个USB接口和一个10/100 以太网接口（A型没有网口），可连接键盘、鼠标和网线，同时拥有视频模拟信号的电视输出接口和HDMI高清视频输出接口，以上部件全部整合在一张仅比信用卡稍大的主板上，具备所有PC的基本功能只需接通电视机和键盘，就能执行如电子表格、文字处理、玩游戏、播放高清视频等诸多功能。 Raspberry Pi B款只提供电脑板，无内存、电源、键盘、机箱或连线。<br>树莓派的生产是通过有生产许可的三家公司Element 14/Premier Farnell、RS Components及Egoman。这三家公司都在网上出售树莓派。你可以在诸如京东、淘宝等国内网站购买到你所想要的树莓派。<br>树莓派基金会提供了基于ARM的Debian和Arch Linux的发行版供大众下载。还计划提供支持Python作为主要编程语言，支持Java、BBC BASIC (通过 RISC OS 映像或者Linux的”Brandy Basic”克隆)、C 和Perl等编程语言.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;树莓派由注册于英国的慈善组织“Raspberry Pi 基金会”开发，Eben·Upton/埃·厄普顿为项目带头人。2012年3月，英国剑桥大学埃本·阿普顿（Eben Epton）正式发售世界上最小的台式机，又称卡片式电脑，外形只有信用卡大小，却具有电脑的所有基本功能，这就
      
    
    </summary>
    
    
    
      <category term="技术 知识" scheme="https://jiebaby.github.io/tags/%E6%8A%80%E6%9C%AF-%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>SSM框架</title>
    <link href="https://jiebaby.github.io/2020/03/09/SSM/"/>
    <id>https://jiebaby.github.io/2020/03/09/SSM/</id>
    <published>2020-03-09T04:20:45.000Z</published>
    <updated>2020-03-11T02:47:44.677Z</updated>
    
    <content type="html"><![CDATA[<p>SSM（Spring+SpringMVC+MyBatis）框架集由Spring、MyBatis两个开源框架整合而成（SpringMVC是Spring中的部分内容）。常作为数据源较简单的web项目的框架。</p><h3 id="SpringMVC"><a href="#SpringMVC" class="headerlink" title="SpringMVC"></a>SpringMVC</h3><p>它用于web层，相当于controller（等价于传统的servlet和struts的action），用来处理用户请求。举个例子，用户在地址栏输入http://网站域名/login，那么springmvc就会拦截到这个请求，并且调用controller层中相应的方法，（中间可能包含验证用户名和密码的业务逻辑，以及查询数据库操作，但这些都不是springmvc的职责），最终把结果返回给用户，并且返回相应的页面（当然也可以只返回json/xml等格式数据）。springmvc就是做前面和后面过程的活，与用户打交道！！</p><h3 id="Spring"><a href="#Spring" class="headerlink" title="Spring"></a>Spring</h3><p>Spring：太强大了，以至于我无法用一个词或一句话来概括它。但与我们平时开发接触最多的估计就是IOC容器，它可以装载bean（也就是我们java中的类，当然也包括service dao里面的），有了这个机制，我们就不用在每次使用这个类的时候为它初始化，很少看到关键字new。另外spring的aop，事务管理等等都是我们经常用到的。</p><h3 id="MyBatis"><a href="#MyBatis" class="headerlink" title="MyBatis"></a>MyBatis</h3><p>MyBatis：如果你问我它跟鼎鼎大名的Hibernate有什么区别？我只想说，他更符合我的需求。第一，它能自由控制sql，这会让有数据库经验的人（当然不是说我啦捂脸）编写的代码能搞提升数据库访问的效率。第二，它可以使用xml的方式来组织管理我们的sql，因为一般程序出错很多情况下是sql出错，别人接手代码后能快速找到出错地方，甚至可以优化原来写的sql。</p><p>pom.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line">$ &lt;project xmlns=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> xmlns:xsi=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span><br><span class="line">xsi:schemaLocation=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"</span>&gt;</span><br><span class="line">&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line">&lt;groupId&gt;com.soecode.ssm&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;ssm&lt;/artifactId&gt;</span><br><span class="line">&lt;packaging&gt;war&lt;/packaging&gt;</span><br><span class="line">&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;</span><br><span class="line">&lt;name&gt;ssm Maven Webapp&lt;/name&gt;</span><br><span class="line">&lt;url&gt;http://github.com/liyifeng1994/ssm&lt;/url&gt;</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">&lt;!-- 单元测试 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.11&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 1.日志 --&gt;</span><br><span class="line">&lt;!-- 实现slf4j接口并整合 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;logback-classic&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.1.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 2.数据库 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;5.1.37&lt;/version&gt;</span><br><span class="line">&lt;scope&gt;runtime&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;c3p0&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;c3p0&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;0.9.1.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- DAO: MyBatis --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.mybatis&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;mybatis&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;3.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.mybatis&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.2.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 3.Servlet web --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;taglibs&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;standard&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.1.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;jstl&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;jstl&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;2.5.4&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;javax.servlet&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;3.1.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 4.Spring --&gt;</span><br><span class="line">&lt;!-- 1)Spring核心 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.springframework&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-core&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.7.RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.springframework&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-beans&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.7.RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.springframework&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-context&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.7.RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- 2)Spring DAO层 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.springframework&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.7.RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.springframework&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-tx&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.7.RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- 3)Spring web --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.springframework&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-web&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.7.RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.springframework&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.7.RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- 4)Spring <span class="built_in">test</span> --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.springframework&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-test&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;4.1.7.RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- redis客户端:Jedis --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;redis.clients&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;jedis&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;2.7.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;protostuff-core&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.0.8&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;protostuff-runtime&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.0.8&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Map工具类 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;commons-collections&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;commons-collections&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;3.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br><span class="line">&lt;build&gt;</span><br><span class="line">&lt;finalName&gt;ssm&lt;/finalName&gt;</span><br><span class="line">&lt;/build&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SSM（Spring+SpringMVC+MyBatis）框架集由Spring、MyBatis两个开源框架整合而成（SpringMVC是Spring中的部分内容）。常作为数据源较简单的web项目的框架。&lt;/p&gt;
&lt;h3 id=&quot;SpringMVC&quot;&gt;&lt;a href=&quot;#Sp
      
    
    </summary>
    
    
    
      <category term="学习  技术" scheme="https://jiebaby.github.io/tags/%E5%AD%A6%E4%B9%A0-%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://jiebaby.github.io/2020/03/08/hello-world/"/>
    <id>https://jiebaby.github.io/2020/03/08/hello-world/</id>
    <published>2020-03-08T08:10:03.277Z</published>
    <updated>2020-03-11T02:47:39.800Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Hello-Hexo"><a href="#Hello-Hexo" class="headerlink" title="Hello Hexo"></a>Hello Hexo</h2><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
