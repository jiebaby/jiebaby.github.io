
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Deep Learning - 卡布诺奇——喵喵喵</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Jay,"> 
    <meta name="description" content="深度学习(DL, Deep Learning)是机器学习(ML, Machine Learning)领域中一个新的研究方向，它被引入机器学习使其更接近于最初的目标——人工智能(AI, Artific,"> 
    <meta name="author" content="Jaybao"> 
    <link rel="alternative" href="atom.xml" title="卡布诺奇——喵喵喵" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
<link rel="stylesheet" href="/css/diaspora.css">

    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
<meta name="generator" content="Hexo 4.2.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">卡布诺奇——喵喵喵</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="icon-home image-icon" href="javascript:;" data-url="https://jiebaby.github.io"></a>
    <div title="播放/暂停" class="icon-play"></div>
    <h3 class="subtitle">Deep Learning</h3>
    <div class="social">
        <!--<div class="like-icon">-->
            <!--<a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
        <!--</div>-->
        <div>
            <div class="share">
                <a title="获取二维码" class="icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">Deep Learning</h1>
        <div class="stuff">
            <span>三月 10, 2020</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E5%AD%A6%E4%B9%A0-%E6%8A%80%E6%9C%AF/" rel="tag">学习 技术</a></li></ul>


        </div>
        <div class="content markdown">
            <p>深度学习(DL, Deep Learning)是机器学习(ML, Machine Learning)领域中一个新的研究方向，它被引入机器学习使其更接近于最初的目标——人工智能(AI, Artificial Intelligence)。<br>深度学习是学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字，图像和声音等数据的解释有很大的帮助。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据。 深度学习是一个复杂的机器学习算法，在语音和图像识别方面取得的效果，远远超过先前相关技术。<br>深度学习在搜索技术，数据挖掘，机器学习，机器翻译，自然语言处理，多媒体学习，语音，推荐和个性化技术，以及其他相关领域都取得了很多成果。深度学习使机器模仿视听和思考等人类的活动，解决了很多复杂的模式识别难题，使得人工智能相关技术取得了很大进步。</p>
<h2 id="十种深度学习算法要点及代码解析"><a href="#十种深度学习算法要点及代码解析" class="headerlink" title="十种深度学习算法要点及代码解析"></a>十种深度学习算法要点及代码解析</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>Python 代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$#Import</span> Library</span><br><span class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></span><br><span class="line">from sklearn import linear_model</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Load Train and Test datasets</span></span><br><span class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></span><br><span class="line">x_train=input_variables_values_training_datasets</span><br><span class="line">y_train=target_variables_values_training_datasets</span><br><span class="line">x_test=input_variables_values_test_datasets</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Create linear regression object</span></span><br><span class="line">linear = linear_model.LinearRegression()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">linear.fit(x_train, y_train)</span><br><span class="line">linear.score(x_train, y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Equation coefficient and Intercept</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Coefficient: n'</span>, linear.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Intercept: n'</span>, linear.intercept_)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= linear.predict(x_test)</span><br></pre></td></tr></table></figure>
<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Load Train and Test datasets</span></span><br><span class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></span><br><span class="line">x_train &lt;- input_variables_values_training_datasets</span><br><span class="line">y_train &lt;- target_variables_values_training_datasets</span><br><span class="line">x_test &lt;- input_variables_values_test_datasets</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">linear &lt;- lm(y_train ~ ., data = x)</span><br><span class="line">summary(linear)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(linear,x_test)</span><br></pre></td></tr></table></figure>

<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>别被它的名字迷惑了！这是一个分类算法而不是一个回归算法。该算法可根据已知的一系列因变量估计离散数值（比方说二进制数值 0 或 1 ，是或否，真或假）。简单来说，它通过将数据拟合进一个逻辑函数来预估一个事件出现的概率。因此，它也被叫做逻辑回归。因为它预估的是概率，所以它的输出值大小在 0 和 1 之间（正如所预计的一样）。</p>
<p>Python代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$#Import</span> Library</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create logistic regression object</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line">model.score(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Equation coefficient and Intercept</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Coefficient: n'</span>, model.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Intercept: n'</span>, model.intercept_)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure>

<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ x &lt;- cbind(x_train,y_train)</span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">logistic &lt;- glm(y_train ~ ., data = x,family=<span class="string">'binomial'</span>)</span><br><span class="line">summary(logistic)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(logistic,x_test)</span><br></pre></td></tr></table></figure>

<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>这个监督式学习算法通常被用于分类问题。令人惊奇的是，它同时适用于分类变量和连续因变量。在这个算法中，我们将总体分成两个或更多的同类群。这是根据最重要的属性或者自变量来分成尽可能不同的组别。</p>
<p>Python代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></span><br><span class="line">from sklearn import tree</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create tree object</span></span><br><span class="line">model = tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>)</span><br><span class="line"><span class="comment"># for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini </span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># model = tree.DecisionTreeRegressor() for regression</span></span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line">model.score(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure>

<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$library</span>(rpart)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># grow tree</span></span><br><span class="line">fit &lt;- rpart(y_train ~ ., data = x,method=<span class="string">"class"</span>)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure>

<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><p>这是一种分类方法。在这个算法中，我们将每个数据在N维空间中用点标出（N是你所有的特征总数），每个特征的值是一个坐标的值。</p>
<p>举个例子，如果我们只有身高和头发长度两个特征，我们会在二维空间中标出这两个变量，每个点有两个坐标（这些坐标叫做支持向量）。</p>
<p>Python代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn import svm</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predic</span></span><br><span class="line">tor) and Y (target) <span class="keyword">for</span> training data <span class="built_in">set</span> and x_test(predictor) of test_dataset</span><br><span class="line"><span class="comment"># Create SVM classification object</span></span><br><span class="line">model = svm.svc()</span><br><span class="line"><span class="comment"># there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.</span></span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line">model.score(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure>
<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$library</span>(e1071)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fit &lt;-svm(y_train ~ ., data = x)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure>


<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>在预示变量间相互独立的前提下，根据 贝叶斯定理 可以得到朴素贝叶斯这个分类方法。用更简单的话来说，一个朴素贝叶斯分类器假设一个分类的特性与该分类的其它特性不相关。举个例子，如果一个水果又圆又红 ， 并且直径大约是 3 英寸，那么这个水果可能会是苹果。即便这些特性互相依赖 ， 或者依赖于别的特性的存在，朴素贝叶斯分类器还是会假设这些特性分别独立地暗示这个水果是个苹果。</p>
<p>朴素贝叶斯模型易于建造，且对于大型数据集非常有用。虽然简单，但是朴素贝叶斯的表现却超越了非常复杂的分类方法。</p>
<p>贝叶斯定理提供了一种从P(c)、P(x)和P(x|c) 计算后验概率 P(c|x) 的方法。</p>
<p>Python代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link</span></span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure>
<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ library(e1071)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fit &lt;-naiveBayes(y_train ~ ., data = x)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure>


<h3 id="KNN（K-–-最近邻算法）"><a href="#KNN（K-–-最近邻算法）" class="headerlink" title="KNN（K – 最近邻算法）"></a>KNN（K – 最近邻算法）</h3><p>该算法可用于分类问题和回归问题。然而，在业界内，K – 最近邻算法更常用于分类问题。K – 最近邻算法是一个简单的算法。它储存所有的案例，通过周围k个案例中的大多数情况划分新的案例。根据一个距离函数，新案例会被分配到它的 K 个近邻中最普遍的类别中去。</p>
<p>这些距离函数可以是欧式距离、曼哈顿距离、明式距离或者是汉明距离。前三个距离函数用于连续函数，第四个函数（汉明函数）则被用于分类变量。如果 K=1，新案例就直接被分到离其最近的案例所属的类别中。有时候，使用 KNN 建模时，选择 K 的取值是一个挑战。</p>
<p>Python代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create KNeighbors classifier object model</span></span><br><span class="line">KNeighborsClassifier(n_neighbors=6)</span><br><span class="line"><span class="comment"># default value for n_neighbors is 5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure>
<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ library(knn)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fit &lt;-knn(y_train ~ ., data = x,k=5)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure>

<h3 id="K-均值算法"><a href="#K-均值算法" class="headerlink" title="K 均值算法"></a>K 均值算法</h3><p>K – 均值算法是一种非监督式学习算法，它能解决聚类问题。使用 K – 均值算法来将一个数据归入一定数量的集群（假设有 k 个集群）的过程是简单的。一个集群内的数据点是均匀齐次的，并且异于别的集群。</p>
<p>还记得从墨水渍里找出形状的活动吗？K – 均值算法在某方面类似于这个活动。观察形状，并延伸想象来找出到底有多少种集群或者总体。</p>
<p>Python代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset</span></span><br><span class="line"><span class="comment"># Create KNeighbors classifier object model</span></span><br><span class="line">k_means = KMeans(n_clusters=3, random_state=0)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure>

<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ library(cluster)</span><br><span class="line">fit &lt;- kmeans(X, 3) <span class="comment"># 5 cluster solution</span></span><br></pre></td></tr></table></figure>

<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>随机森林是表示决策树总体的一个专有名词。在随机森林算法中，我们有一系列的决策树（因此又名“森林”）。为了根据一个新对象的属性将其分类，每一个决策树有一个分类，称之为这个决策树“投票”给该分类。这个森林选择获得森林里（在所有树中）获得票数最多的分类。</p>
<p>每棵树是像这样种植养成的：</p>
<p>如果训练集的案例数是 N，则从 N 个案例中用重置抽样法随机抽取样本。这个样本将作为“养育”树的训练集。<br>假如有 M 个输入变量，则定义一个数字 m&lt;&lt;M。m 表示，从 M 中随机选中 m 个变量，这 m 个变量中最好的切分会被用来切分该节点。在种植森林的过程中，m 的值保持不变。<br>尽可能大地种植每一棵树，全程不剪枝。</p>
<p>Python</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create Random Forest object</span></span><br><span class="line">model= RandomForestClassifier()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure>

<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ library(randomForest)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fit &lt;- randomForest(Species ~ ., x,ntree=500)</span><br><span class="line">summary(fit)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= predict(fit,x_test)</span><br></pre></td></tr></table></figure>

<h3 id="降维算法"><a href="#降维算法" class="headerlink" title="降维算法"></a>降维算法</h3><p>在过去的 4 到 5 年里，在每一个可能的阶段，信息捕捉都呈指数增长。公司、政府机构、研究组织在应对着新资源以外，还捕捉详尽的信息。</p>
<p>举个例子：电子商务公司更详细地捕捉关于顾客的资料：个人信息、网络浏览记录、他们的喜恶、购买记录、反馈以及别的许多信息，比你身边的杂货店售货员更加关注你。</p>
<p>Python代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn import decomposition</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have training and test data set as train and test</span></span><br><span class="line"><span class="comment"># Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)</span></span><br><span class="line"><span class="comment"># For Factor analysis</span></span><br><span class="line"><span class="comment">#fa= decomposition.FactorAnalysis()</span></span><br><span class="line"><span class="comment"># Reduced the dimension of training dataset using PCA</span></span><br><span class="line">train_reduced = pca.fit_transform(train)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Reduced the dimension of test dataset</span></span><br><span class="line">test_reduced = pca.transform(<span class="built_in">test</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#For more detail on this, please refer  this link.</span></span><br></pre></td></tr></table></figure>

<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ library(stats)</span><br><span class="line">pca &lt;- princomp(train, cor = TRUE)</span><br><span class="line">train_reduced  &lt;- predict(pca,train)</span><br><span class="line">test_reduced  &lt;- predict(pca,<span class="built_in">test</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Gradient-Boosting-和-AdaBoost-算法"><a href="#Gradient-Boosting-和-AdaBoost-算法" class="headerlink" title="Gradient Boosting 和 AdaBoost 算法"></a>Gradient Boosting 和 AdaBoost 算法</h3><p>当我们要处理很多数据来做一个有高预测能力的预测时，我们会用到 GBM 和 AdaBoost 这两种 boosting 算法。boosting 算法是一种集成学习算法。它结合了建立在多个基础估计值基础上的预测结果，来增进单个估计值的可靠程度。这些 boosting 算法通常在数据科学比赛如 Kaggl、AV Hackathon、CrowdAnalytix 中很有效。</p>
<p>Python代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#Import Library</span></span><br><span class="line">from sklearn.ensemble import GradientBoostingClassifier</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></span><br><span class="line"><span class="comment"># Create Gradient Boosting Classifier object</span></span><br><span class="line">model= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Train the model using the training sets and check score</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Predict Output</span></span><br><span class="line">predicted= model.predict(x_test)</span><br></pre></td></tr></table></figure>

<p>R代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ library(caret)</span><br><span class="line">x &lt;- cbind(x_train,y_train)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fitting model</span></span><br><span class="line">fitControl &lt;- trainControl( method = <span class="string">"repeatedcv"</span>, number = 4, repeats = 4)</span><br><span class="line">fit &lt;- train(y ~ ., data = x, method = <span class="string">"gbm"</span>, trControl = fitControl,verbose = FALSE)</span><br><span class="line">predicted= predict(fit,x_test,<span class="built_in">type</span>= <span class="string">"prob"</span>)[,2]</span><br></pre></td></tr></table></figure>
<p>GradientBoostingClassifier 和随机森林是两种不同的 boosting 树分类器。</p>
<p>如果你想要掌握机器学习，那就立刻开始吧。做做练习，理性地认识整个过程，应用这些代码，并感受乐趣吧！</p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="true">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='http://link.hhtjim.com/163/5146554.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='http://link.hhtjim.com/qq/001faIUs4M2zna.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
        data-ae='true'
        data-ci=''
        data-cs=''
        data-r=''
        data-o=''
        data-a=''
        data-d='true'
    >查看评论</div>


    </div>
    
        <div class='side'>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#十种深度学习算法要点及代码解析"><span class="toc-number">1.</span> <span class="toc-text">十种深度学习算法要点及代码解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归"><span class="toc-number">1.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#逻辑回归"><span class="toc-number">1.2.</span> <span class="toc-text">逻辑回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#决策树"><span class="toc-number">1.3.</span> <span class="toc-text">决策树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#支持向量机"><span class="toc-number">1.4.</span> <span class="toc-text">支持向量机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#朴素贝叶斯"><span class="toc-number">1.5.</span> <span class="toc-text">朴素贝叶斯</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KNN（K-–-最近邻算法）"><span class="toc-number">1.6.</span> <span class="toc-text">KNN（K – 最近邻算法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-均值算法"><span class="toc-number">1.7.</span> <span class="toc-text">K 均值算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#随机森林"><span class="toc-number">1.8.</span> <span class="toc-text">随机森林</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#降维算法"><span class="toc-number">1.9.</span> <span class="toc-text">降维算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Boosting-和-AdaBoost-算法"><span class="toc-number">1.10.</span> <span class="toc-text">Gradient Boosting 和 AdaBoost 算法</span></a></li></ol></li></ol>
        </div>
    
</div>


    </div>
</div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"search":null,"path":"search.xml","field":"post","format":"html","limit":10000});</script></body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>





</html>
